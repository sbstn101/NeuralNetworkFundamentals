{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of simple MLP using only NumPy\n",
    "\n",
    "This notebook implements a simple MLP from scratch using only NumPy. This is done for educational purposes and the implementations are based on relevant research papers, no \"cheating\" by looking at example code or utilizing chatGPT-like tools were involved. This also means that there might be some errors but the classes will be updated in the future as any issues come up. \n",
    "\n",
    "The design of the classes is modular to allow for easy modification and building of more complex networks using the same building blocks in the future.\n",
    "\n",
    "The training example we will start with is still unkonwn :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neuron\n",
    "\n",
    "In this part we will define the atomic unit of a artificial neural network, the neuron. A central part of the neuron is the activation function and since modularity is a important feature of the project, we will begin by defining a class for activation functions, so that different ones can be utilized if wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "We now define a class structure for activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, z):\n",
    "        pass\n",
    "\n",
    "    def derivative(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we definine the ReLU activation function to be used by the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0 if self.z > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we define one that does nothing to enable linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoActivation(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        return z\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the neural network by defining a single neuron. We want to use ReLU non-linearity (f(x) = max(0,x)) so we will use the weight initialization method introduced in https://arxiv.org/pdf/1502.01852.pdf.\n",
    "\n",
    "The article mentions that using equation 10 or 14 gives the same results so let's opt to using eq.10   $\\frac{1}{2}n_{l}Var[w_{l}] = 1,   \\forall{l}.$   Which leads to a zero mean gaussian with standard deviation of $\\sqrt{2/n_{l}}$. Here w refers to the weights and n to the number of connections.\n",
    "\n",
    "The bias term is initialized to zero.\n",
    "\n",
    "The implementation of backpropagation heavily relies on the amazing content of 3blue1brown https://www.3blue1brown.com/lessons/backpropagation-calculus. In essense, backpropagation is one method with which the gradient vector of the cost function of the ANN can be calculated. It is based on the chain-rule. Each neuron will calculate 3 different gradients of the cost function using the chain rule. The gradient with respect to the weights, bias and the activations. It will then pass on the gradient with respect to the activations to the next layer of neurons, and the cycle continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    def __init__(self, input_connections=1, activation_func=None):\n",
    "        if not isinstance(activation_func, Activation):\n",
    "            raise TypeError(\"An activation function must be defined and be a subclass to the Activation class\")\n",
    "        \n",
    "        self.weights = np.random.normal(loc=0, scale=math.sqrt(2/input_connections), size=(input_connections))\n",
    "        self.b = 0\n",
    "        self.activation_f = activation_func\n",
    "        self.activations = None\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        assert inputs.shape == self.weights.shape\n",
    "\n",
    "        self.activations = inputs\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        self.z = z\n",
    "        output = self.activation_f.forward(z)\n",
    "        return(output)\n",
    "    \n",
    "    def backward(self, dC_dout):\n",
    "        dz_din = self.weights\n",
    "        dz_dw = self.activations\n",
    "        dz_db = 1\n",
    "        dout_dz = self.activation_f.derivative()\n",
    "        dC_dz = dC_dout * dout_dz\n",
    "        weight_grad = dC_dz * dz_dw\n",
    "        bias_grad = dC_dz * dz_db\n",
    "        act_grad = dC_dz * dz_din\n",
    "        return (weight_grad, bias_grad, act_grad)\n",
    "\n",
    "    # Functions for setting the weigts and biases from outside the class\n",
    "    def set_weights(self, new_weights):\n",
    "        self.weights = new_weights\n",
    "\n",
    "    def set_biases(self, new_b):\n",
    "        self.b = new_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the MLP\n",
    "\n",
    "Now it is time to build the network using the neuron class as building blocks.\n",
    "\n",
    "We define a simple network with one input layer, two hidden layers and one output layer. The input layer will have [insert how many when the task has been defined] nodes, the hidden layers will have 128 nodes each and the output layer will have [insert how many again when clear] nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP():\n",
    "    def __init__(self, num_input=1, num_output=1, num_hidden=128):\n",
    "        self.hidden_1 = [Neuron(num_input, ReLU()) for _ in range(num_hidden)]\n",
    "        self.hidden_2 = [Neuron(num_hidden, ReLU()) for _ in range(num_hidden)]\n",
    "        self.output_layer = [Neuron(num_hidden, NoActivation()) for _ in range(num_output)]\n",
    "        self.weights = \n",
    "        self.biases = \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # TODO: Define\n",
    "    \n",
    "    def backward(self):\n",
    "        # TODO: Define\n",
    "\n",
    "    # Functions for setting the weigts and biases from outside the class\n",
    "    def set_weights(self, new_weights):\n",
    "        self.weights = new_weights\n",
    "\n",
    "    def set_biases(self, new_biases):\n",
    "        self.biases = new_biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimization\n",
    "\n",
    "Let's continue with implementing the Adam optimization algorithm which optimizes Stochastic Gradient Decent, based on the Adam paper https://arxiv.org/pdf/1412.6980.pdf. Adam is an algorithm for first order gradient base optimization of stochastic objective functions. Name comes from adaptive moment estimation. The authors propose a good initialization for the hyperparameters to be:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha &= 0.001\\\\\n",
    "\\beta_1 &= 0.9\\\\\n",
    "\\beta_2 &= 0.999\\\\\n",
    "\\epsilon &= 10^{-8}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The update function follows the proposed algorithm 1 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, network, lr = 0.001, b1 = 0.9, b2 = 0.999, epsilon = 10^(-8)):\n",
    "        self.num_weights = len(network.weights)\n",
    "        self.net = network\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.e = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, weight_g, bias_g):\n",
    "        weights = self.net.weights\n",
    "        biases = self.net.biases\n",
    "        theta = np.append(weights, biases)\n",
    "\n",
    "        # One step of while loop in Adam algorithm following the paper:\n",
    "        self.t += 1\n",
    "        gt = np.append(weight_g,bias_g)\n",
    "        mt = self.b1*self.m + (1-self.b1)*gt\n",
    "        vt = self.b2*self.v + (1-self.b2)*gt**2\n",
    "        mt_hat = mt/(1-self.b1**self.t)\n",
    "        vt_hat = vt/(1-self.b2**self.t)\n",
    "        thetat = theta - self.lr*mt_hat/(np.sqrt(vt_hat) + self.e)\n",
    "\n",
    "        # Update network\n",
    "        weight_t, bias_t = np.split(thetat, [self.num_weights])\n",
    "        self.net.set_weights(weight_t)\n",
    "        self.net.set_biases(bias_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
