{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of simple MLP using only NumPy\n",
    "\n",
    "This notebook implements a simple MLP from scratch using only NumPy. This is done for educational purposes and the implementations are based on relevant research papers, no \"cheating\" by looking at example code or utilizing chatGPT-like tools were involved. This also means that there might be some errors but the classes will be updated in the future as any issues come up. \n",
    "\n",
    "The design of the classes is modular to allow for easy modification and building of more complex networks using the same building blocks in the future.\n",
    "\n",
    "The training example we will start with is still unkonwn :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"c:/Users/sebu1/OneDrive/Github Projects/forwardforward/forwardforward\")\n",
    "import CellularAutomata\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start building the neural network by defining a single neuron. We want to use ReLU non-linearity (f(x) = max(0,x)) so we will use the weight initialization method introduced in https://arxiv.org/pdf/1502.01852.pdf. \n",
    "\n",
    "The article mentions that using equation 10 or 14 gives the same results so let's opt to using eq.10   $\\frac{1}{2}n_{l}Var[w_{l}] = 1,   \\forall{l}.$   Which leads to a zero mean gaussian with standard deviation of $\\sqrt{2/n_{l}}$. Here w refers to the weights and n to the number of connections.\n",
    "\n",
    "The bias term is initialized to zero.\n",
    "\n",
    "The implementation of backpropagation heavily relies on the amazing content of 3blue1brown https://www.3blue1brown.com/lessons/backpropagation-calculus. In essense, backpropagation is one method with which the gradient vector of the cost function of the ANN can be calculated. It is based on the chain-rule. Each neuron will calculate 3 different gradients of the cost function using the chain rule. The gradient with respect to the weights, bias and the activations. It will then pass on the gradient with respect to the activations to the next layer of neurons, and the cycle continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "class Neuron():\n",
    "    def __init__(self, input_connections=1, activation_func=lambda x: np.maximum(0,x)):\n",
    "        self.weights = np.random.normal(loc=0, scale=math.sqrt(2/input_connections), size=(input_connections))\n",
    "        self.b = 0\n",
    "        self.activation_f = activation_func\n",
    "        self.activations = None\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        assert inputs.shape == self.weights.shape\n",
    "\n",
    "        self.activations = inputs\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        self.z = z\n",
    "        output = self.activation_f(z)\n",
    "        return(output)\n",
    "    \n",
    "    def backward(self, dC_dout):\n",
    "        dz_din = self.weights\n",
    "        dz_dw = self.activations\n",
    "        dz_db = 1\n",
    "        dout_dz = 1.0 if self.z > 0 else 0.0\n",
    "        dC_dz = dC_dout * dout_dz\n",
    "        weight_grad = dC_dz * dz_dw\n",
    "        bias_grad = dC_dz * dz_db\n",
    "        act_grad = dC_dz * dz_din\n",
    "        return (weight_grad, bias_grad, act_grad)\n",
    "\n",
    "    # Functions for setting the weigts and biases from outside the class\n",
    "    def set_weights(self, new_weights):\n",
    "        self.weights = new_weights\n",
    "\n",
    "    def set_biases(self, new_b):\n",
    "        self.b = new_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with implementing the Adam optimization algorithm which optimizes Stochastic Gradient Decent, based on the Adam paper https://arxiv.org/pdf/1412.6980.pdf. Adam is an algorithm for first order gradient base optimization of stochastic objective functions. Name comes from adaptive moment estimation. The authors propose a good initialization for the hyperparameters to be:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha &= 0.001\\\\\n",
    "\\beta_1 &= 0.9\\\\\n",
    "\\beta_2 &= 0.999\\\\\n",
    "\\epsilon &= 10^{-8}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The update function follows the proposed algorithm 1 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, network, lr = 0.001, b1 = 0.9, b2 = 0.999, epsilon = 10^(-8)):\n",
    "        self.num_weights = len(network.weights)\n",
    "        self.net = network\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.e = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, weight_g, bias_g):\n",
    "        weights = self.net.weights\n",
    "        biases = self.net.biases\n",
    "        theta = np.append(weights, biases)\n",
    "\n",
    "        # One step of while loop in Adam algorithm following the paper:\n",
    "        self.t += 1\n",
    "        gt = np.append(weight_g,bias_g)\n",
    "        mt = self.b1*self.m + (1-self.b1)*gt\n",
    "        vt = self.b2*self.v + (1-self.b2)*gt**2\n",
    "        mt_hat = mt/(1-self.b1**self.t)\n",
    "        vt_hat = vt/(1-self.b2**self.t)\n",
    "        thetat = theta - self.lr*mt_hat/(np.sqrt(vt_hat) + self.e)\n",
    "\n",
    "        # Update network\n",
    "        weight_t, bias_t = np.split(thetat, [self.num_weights])\n",
    "        self.net.set_weights(weight_t)\n",
    "        self.net.set_biases(bias_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
